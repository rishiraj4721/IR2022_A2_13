{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "e5a7a41d-bfc7-4ee0-ab97-9f7cd3fab8a3",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e148f4a8",
    "execution_start": 1649526412560,
    "execution_millis": 828,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 539.4000244140625
   },
   "source": "from os import listdir\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport pandas as pd\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nimport nltk\n\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\nstop_words = stopwords.words('english')",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /root/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00001-e2a978f1-f504-42e3-ac17-eb6ae3a45c03",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "225e9c0",
    "execution_start": 1649526413242,
    "execution_millis": 1,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 854.5999755859375
   },
   "source": "def preprocessing(text_file):\n    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")    # remove punctuations\n    tokens = tokenizer.tokenize(text_file)    # token the text\n\n    tokens = [token.lower() for token in tokens]    # lower case\n    tokens = [word for word in tokens if not word in stop_words]    # remove stop words\n\n    stemmer = PorterStemmer()    #stemming tokens\n    tokens = [stemmer.stem(word) for word in tokens]\n    \n    lemmatizer = WordNetLemmatizer()   #lemmatizing tokens\n    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n    \n    return tokens\n\n# get a list of all files in the directory\ndef getFilesList(folder):\n    dir_list = listdir(folder)    #subfolders\n    files = {}\n    for i in dir_list:\n        path = folder+i+\"/\"\n        diri = listdir(path)    #files per subfolder\n        files[i] = []\n        for j in diri:\n            filepath = path+j\n            files[i].append(filepath)    #storing full path of file\n    return files\n\ndef files_read_preprocess(folderpath):\n    flist = getFilesList(folderpath)\n    alltextfromfiles = []\n    textlabels = []\n    for folder in flist:\n        for file in flist[folder]:\n            text = open(file,'r',encoding='ISO-8859-1').read()\n            tokens = preprocessing(text)\n            alltextfromfiles.append(tokens)\n            textlabels.append(folder)    #corresponding label of text\n\n    print(len(alltextfromfiles))\n    print(len(textlabels))\n\n    dataset = pd.DataFrame([alltextfromfiles,textlabels]).T\n    dataset.to_pickle(\"q3_preprocessed\")",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "cdbd61ad877740fb954772eac8dc78aa",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "3d5f1905",
    "execution_start": 1649526413261,
    "execution_millis": 1,
    "owner_user_id": "7aceb5e9-694e-489d-827f-fa7d6774ae97",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1304.5999755859375,
    "deepnote_output_heights": [
     79
    ]
   },
   "source": "def tficf_feature_select(x_train,top_k):\n    corpus = []\n    for i in class_dict:\n        corpus += class_dict[i]    # all the text from all classes\n\n    CF = {}\n    for i in class_dict:\n        for word in class_dict[i]:\n            if word not in CF:\n                CF[word] = {}\n            CF[word][i] = 1    # storing all classes where word tokens is present\n    #print(len(CF))\n\n    tf_icf = {}\n    N = len(uniq_labels)\n    counter=Counter(corpus)\n    word_count=len(corpus)\n\n    for token in set(corpus):\n        tf = counter[token]/word_count    # term frequency\n        try:\n            cf = len(CF[token])    # class frequency\n        except:\n            pass\n        icf = np.log(N/(cf))    # inverse class frequency\n        tf_icf[token]=icf*tf    # TF-ICF for token\n\n    # selecting top k tokens with highest tf-icf values\n    sorted_tf_icf = sorted(tf_icf.items(),key=lambda x:x[1],reverse=True)[:top_k]\n    return [i[0] for i in sorted_tf_icf]\n\ndef getCountFreqTopKFeatures(feature_list,labels):\n    class_frequency = {}\n    class_count = {}\n    counter = 0\n    for i in labels:\n        current_count = len(Counter(class_dict[i]))\n        class_count[i] = current_count\n        counter += current_count\n        ll = Counter(class_dict[i])\n        for j in ll:\n            class_frequency[i, j] = ll[j]\n    return class_frequency,class_count\n\ndef Naive_Bayes(class_frequency,class_count,x_test,y_test):\n    actual = []\n    predicted = []\n\n    for i in range(x_test.shape[0]):\n        classes_word_prob = []\n        actual.append(y_test[i])\n\n        for labels in uniq_labels:\n            word_prob = 0\n\n            for word in x_test[0][i]:\n                t1,t2 = 0,class_count[labels]\n                try:\n                    t1 = class_frequency[labels,word]\n                except: pass\n                t3 = (t1+1)/(t2+unique_words_count)\n                word_prob += np.log(t3)\n\n            classes_word_prob.append(word_prob)\n        predicted.append(uniq_labels[np.argmax(classes_word_prob)])\n\n    return actual,predicted\n\ndef Naive_Bayes2(class_frequency,class_count,x_test,y_test):\n    ",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "9fce7b1a16014013bcb27259802876dd",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "36d2a69c",
    "execution_start": 1649526413270,
    "execution_millis": 13974,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1351.5999755859375,
    "deepnote_output_heights": [
     328,
     328
    ]
   },
   "source": "folderpath = \"20_newsgroups/\"\n# dataset = files_read_preprocess(folderpath)\ndataset = pd.read_pickle(\"q3_preprocessed\")\n# print(dataset)\nuniq_labels = listdir(folderpath)\n\ny = dataset.pop(1)\nX = dataset\nsplits = [0.2,0.3,0.5]\nfeatures_top_k = 100\n\nfor splitsize in splits:\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=splitsize, random_state=42)\n    X_train = X_train.reset_index(drop=True)\n    y_train = y_train.reset_index(drop=True)\n    X_test = X_test.reset_index(drop=True)\n    y_test = y_test.reset_index(drop=True)\n\n    class_dict = {}\n    for i in range(X_train.shape[0]):\n        if y_train[i] not in class_dict:\n            class_dict[y_train[i]] = []\n        class_dict[y_train[i]] += X_train[0][i]\n\n    unique_words = set()\n    for i in class_dict:\n        unique_words = unique_words | set(class_dict[i])\n    unique_words_count = len(unique_words)\n\n    sorted_feature_list = tficf_feature_select(X_train,features_top_k)\n    # print(sorted_feature_list)\n    class_freq,class_count = getCountFreqTopKFeatures(sorted_feature_list,uniq_labels)\n    actual,predicted = Naive_Bayes(class_freq,class_count,X_test,y_test)\n\n    print(\"Train:Test =\",str(int((1-splitsize)*100))+\":\"+str(int(splitsize*100)))\n    acc = accuracy_score(actual,predicted)\n    print(\"Accuracy =\",acc)\n    confusion = confusion_matrix(actual,predicted)\n    print(\"Confusion Matrix\")\n    print(confusion)\n    print()",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": "Train:Test = 80:20\nAccuracy = 0.971\nConfusion Matrix\n[[207   1   1   0   1]\n [  0 215   0   0   1]\n [  5   1 175   4   7]\n [  2   1   1 185   1]\n [  1   1   0   1 189]]\n\nTrain:Test = 70:30\nAccuracy = 0.9713333333333334\nConfusion Matrix\n[[295   1   2   1   2]\n [  1 312   0   0   3]\n [  5   1 269   4  11]\n [  3   1   1 300   4]\n [  1   1   0   1 281]]\n\nTrain:Test = 50:50\nAccuracy = 0.9744\nConfusion Matrix\n[[475   1   0   1   1]\n [  2 501   1   0   2]\n [ 13   2 462   5  10]\n [ 10   3   1 494   8]\n [  1   1   0   2 504]]\n\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=5be6439c-c857-493e-9eef-529c4c49e2bf' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "8f1e2057-2fee-4b81-8034-42aa0b9a6336",
  "interpreter": {
   "hash": "3a0c17cf44ae63ecae8bbfe3dd930ce5cd52d8df6fc82844b03d40572a42217b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 2
 }
}