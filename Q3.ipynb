{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "e5a7a41d-bfc7-4ee0-ab97-9f7cd3fab8a3",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "63d58b98",
    "execution_start": 1649258858352,
    "execution_millis": 177,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 503.3999938964844
   },
   "source": "from os import listdir\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nimport nltk\n\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\nstop_words = stopwords.words('english')",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /root/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00001-e2a978f1-f504-42e3-ac17-eb6ae3a45c03",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "cb384f32",
    "execution_start": 1649258765804,
    "execution_millis": 16,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 854.5999755859375
   },
   "source": "def preprocessing(text_file):\n    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")    # remove punctuations\n    tokens = tokenizer.tokenize(text_file)    # token the text\n\n    tokens = [token.lower() for token in tokens]    # lower case\n    tokens = [word for word in tokens if not word in stop_words]    # remove stop words\n\n    stemmer = PorterStemmer()    #stemming tokens\n    tokens = [stemmer.stem(word) for word in tokens]\n    \n    lemmatizer = WordNetLemmatizer()   #lemmatizing tokens\n    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n    \n    return tokens\n\n# get a list of all files in the directory\ndef getFilesList(folder):\n    dir_list = listdir(folder)    #subfolders\n    files = {}\n    for i in dir_list:\n        path = folder+i+\"/\"\n        diri = listdir(path)    #files per subfolder\n        files[i] = []\n        for j in diri:\n            filepath = path+j\n            files[i].append(filepath)    #storing full path of file\n    return files\n\ndef files_read_preprocess(folderpath):\n    flist = getFilesList(folderpath)\n    alltextfromfiles = []\n    textlabels = []\n    for folder in flist:\n        for file in flist[folder]:\n            text = open(file,'r',encoding='ISO-8859-1').read()\n            tokens = preprocessing(text)\n            alltextfromfiles.append(tokens)\n            textlabels.append(folder)    #corresponding label of text\n\n    print(len(alltextfromfiles))\n    print(len(textlabels))\n\n    dataset = pd.DataFrame([alltextfromfiles,textlabels]).T\n    dataset.to_pickle(\"q3_preprocessed\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "dc02dbe93b6d4c76b4be3589ae5da63c",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 389.6000061035156
   },
   "source": "train_prior = Counter(y_train)\nprint(train_prior)\n\nclass_dict = {}\nfor i in range(X_train.shape[0]):\n    if y_train[i] not in class_dict:\n        class_dict[y_train[i]] = []\n    class_dict[y_train[i]] += X_train[i]\n\nuniq_labels = listdir(folderpath)\nclass_freq,class_count = termfrequency_tf(uniq_labels,class_dict)\n\nunique_words = set()\nfor i in class_dict:\n    unique_words = unique_words | set(class_dict[i])\n\nunique_words_count = len(unique_words)\nprint(len(class_frequency))\nprint(class_count)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "cdbd61ad877740fb954772eac8dc78aa",
    "tags": [],
    "owner_user_id": "7aceb5e9-694e-489d-827f-fa7d6774ae97",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1181.5999755859375
   },
   "source": "def tficf_feature_select(x_train,y_train,top_k):\n    corpus = []\n    for i in class_dict:\n        corpus += class_dict[i]\n\n    # cf = {}\n    # n = 0\n    \n    # for i in class_dict:\n    #     for word in class_dict[i]:\n    #         if word not in cf:\n    #             cf[word] = {}\n    #         cf[word].add(n)\n    #     n += 1\n\n    # for word in cf:\n    #     cf[word] = len(cf[word])\n    \n    # tficf = {}\n    # sizetrain = train.shape[0]\n\n    # counter = Counter(corpus)\n    # word_count = len(corpus)\n\n    # for token in set(corpus):\n    #     cf = \n\n\ndef getCountFreqTopKFeatures(feature_list):\n    class_frequency = {}\n    class_count = {}\n    counter = 0\n    for i in labels:\n        current_count = len(Counter(feature_list[i]))\n        class_count[i] = current_count\n        counter += current_count\n        ll = Counter(feature_list[i])\n        for j in ll:\n            class_frequency[i, j] = ll[j]\n        \n    return class_frequency,class_count\n\ndef Naive_bayes(class_frequency,class_count,x_test,y_test):\n    actual = []\n    predicted = []\n\n    for i in range(x_test.shape[0]):\n        classes_word_prob = []\n        actual.append(y_test[i])\n\n        for labels in uniq_labels:\n            word_prob = 0\n            for word in x_test[i]:\n                t1,t2 = 0,class_count[labels]\n                if labels,word in class_frequency:\n                    t1 = class_frequency[labels,word]\n                    \n                t3 = (t1+1)/(t2+unique_words_count)\n                word_prob += np.log(t2)\n            classes_word_prob.append(word_prob)\n        predicted.append(uniq_labels[np.argmax(classes_word_prob)])\n\n    return actual,predicted",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "9fce7b1a16014013bcb27259802876dd",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "76b27c3b",
    "execution_start": 1649259018246,
    "execution_millis": 565,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 713.4000244140625
   },
   "source": "folderpath = \"20_newsgroups/\"\n# dataset = files_read_preprocess(folderpath)\ndataset = pd.read_pickle(\"q3_preprocessed\")\n# print(dataset)\n\ny = dataset.pop(1)\nX = dataset\nsplits = [0.2,0.3,0.5]\n\nfor splitsize in splits:\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=splitsize, random_state=42)\n    \n    sorted_feature_list = tficf_feature_select(X_train,y_train,10)\n    class_frequecy1,class_count1 = getCountFreqTopKFeatures(sorted_feature_list)\n    actual,predicted = Naive_Bayes(class_frequecy1,class_count1)\n\n    acc = accuracy_score(actual,predicted)\n    print(acc)\n    confusion = confusion_matrix(actual,predicted)\n    print(confusion)",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "                                                      0                 1\n0     [path, cantaloup, srv, c, cmu, edu, da, news, ...  rec.sport.hockey\n1     [newsgroup, rec, sport, hockey, path, cantalou...  rec.sport.hockey\n2     [path, cantaloup, srv, c, cmu, edu, da, news, ...  rec.sport.hockey\n3     [newsgroup, rec, sport, hockey, path, cantalou...  rec.sport.hockey\n4     [newsgroup, rec, sport, hockey, path, cantalou...  rec.sport.hockey\n...                                                 ...               ...\n4995  [xref, cantaloup, srv, c, cmu, edu, sci, med, ...           sci.med\n4996  [path, cantaloup, srv, c, cmu, edu, rochest, u...           sci.med\n4997  [xref, cantaloup, srv, c, cmu, edu, sci, med, ...           sci.med\n4998  [path, cantaloup, srv, c, cmu, edu, crabappl, ...           sci.med\n4999  [xref, cantaloup, srv, c, cmu, edu, sci, med, ...           sci.med\n\n[5000 rows x 2 columns]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=5be6439c-c857-493e-9eef-529c4c49e2bf' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "8f1e2057-2fee-4b81-8034-42aa0b9a6336",
  "interpreter": {
   "hash": "3a0c17cf44ae63ecae8bbfe3dd930ce5cd52d8df6fc82844b03d40572a42217b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 2
 }
}